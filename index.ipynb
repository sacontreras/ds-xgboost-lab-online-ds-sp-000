{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Lab\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this lab, we'll install the popular [XGBoost Library](http://xgboost.readthedocs.io/en/latest/index.html) and explore how to use this popular boosting model to classify different types of wine using the [Wine Quality Dataset](https://archive.ics.uci.edu/ml/datasets/wine+quality) from the UCI Machine Learning Dataset Repository.  \n",
    "\n",
    "### Step 1: Install XGBoost\n",
    "\n",
    "The XGBoost model is not currently included in scikit-learn, so we'll have to install it on our own.  \n",
    "\n",
    "**Install the library using `conda install py-xgboost`.**"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to import everything we'll need for this lab. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we'll be using for this lab is currently stored in the file `winequality-red.csv`.  \n",
    "\n",
    "In the cell below, use pandas to import the dataset into a dataframe, and inspect the head of the dataframe to ensure everything loaded correctly. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('winequality-red.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, our target variable will be `quality` .  That makes this a multiclass classification problem. Given the data in the columns from `fixed_acidity` through `alcohol`, we'll predict the `quality` of the wine.  \n",
    "\n",
    "This means that we need to store our target variable separately from the dataset, and then split the data and labels into training and testing sets that we can use for cross-validation. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Store the `quality` column in the `labels` variable and then remove the column from our dataset.  \n",
    "* Create a `StandardScaler` object and scale the data using the `fit_transform()` method.\n",
    "* Split the data into training and testing sets using the appropriate method from sklearn.  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.quality\n",
    "labels_removed_df = df.drop('quality', axis=1)\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(labels_removed_df)\n",
    "\n",
    "# Calculate X_train, X_test, y_train, y_test \n",
    "SEED = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_df, labels, test_size=0.25, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared our data for modeling, we can use XGBoost to build a model that can accurately classify wine quality based on the features of the wine!\n",
    "\n",
    "The API for xgboost is purposefully written to mirror the same structure as other models in scikit-learn.  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training Accuracy: 80.9%\nValidation accuracy: 63.5%\n"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(random_state=SEED)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "training_preds = clf.predict(X_train)\n",
    "val_preds = clf.predict(X_test)\n",
    "training_accuracy = accuracy_score(y_train, training_preds)\n",
    "val_accuracy = accuracy_score(y_test, val_preds)\n",
    "\n",
    "print(\"Training Accuracy: {:.4}%\".format(training_accuracy * 100))\n",
    "print(\"Validation accuracy: {:.4}%\".format(val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning XGBoost\n",
    "\n",
    "Our model had somewhat lackluster performance on the testing set compared to the training set, suggesting the model is beginning to overfit the training data.  Let's tune the model to increase the model performance and prevent overfitting. \n",
    "\n",
    "For a full list of model parameters, see the[XGBoost Documentation](http://xgboost.readthedocs.io/en/latest/parameter.html).\n",
    "\n",
    "Many of the parameters we'll be tuning are parameters we've already encountered when working with Decision Trees, Random Forests, and Gradient Boosted Trees.  \n",
    "\n",
    "Examine the tunable parameters for XGboost, and then fill in appropriate values for the `param_grid` dictionary in the cell below. Put values you want to test out  for each parameter inside the corresponding arrays in `param_grid`.  \n",
    "\n",
    "**_NOTE:_** Remember, `GridSearchCV` finds the optimal combination of parameters through an exhaustive combinatoric search.  If you search through too many parameters, the model will take forever to run! For the sake of time, we recommend trying no more than 3 values per parameter for the following steps.  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"learning_rate\": [0.1, 0.3, 0.6, 1.0],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.1, 0.5, 1.0],\n",
    "    'n_estimators': [10, 30, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have constructed our `params` dictionary, create a `GridSearchCV` object in the cell below and use it to iterate tune our XGBoost model.  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Testing Accuracy: 63.89%\n\nGrid Search found the following optimal parameters: \nlearning_rate: 0.1\nmax_depth: 6\nmin_child_weight: 1\nn_estimators: 100\nsubsample: 0.5\n"
    }
   ],
   "source": [
    "K = 3\n",
    "clf = XGBClassifier(random_state=SEED)\n",
    "grid_clf = GridSearchCV(clf, param_grid, cv=K)\n",
    "grid_clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Testing Accuracy: {grid_clf.best_score_ :.2%}\")\n",
    "print(\"\")\n",
    "best_parameters = grid_clf.best_params_\n",
    "print(\"Grid Search found the following optimal parameters: \")\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "training_preds = None\n",
    "val_preds = None\n",
    "training_accuracy = None\n",
    "val_accuracy = None\n",
    "\n",
    "# print(\"\")\n",
    "# print(\"Training Accuracy: {:.4}%\".format(training_accuracy * 100))\n",
    "# print(\"Validation accuracy: {:.4}%\".format(val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training Accuracy: 99.17%\nValidation accuracy: 68.0%\n"
    }
   ],
   "source": [
    "clf = XGBClassifier(\n",
    "    learning_rate=best_parameters['learning_rate'],\n",
    "    max_depth=best_parameters['max_depth'],\n",
    "    min_child_weight=best_parameters['min_child_weight'],\n",
    "    subsample=best_parameters['min_child_weight'],\n",
    "    n_estimators=best_parameters['n_estimators'],\n",
    "    random_state=SEED\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "training_preds = clf.predict(X_train)\n",
    "val_preds = clf.predict(X_test)\n",
    "training_accuracy = accuracy_score(y_train, training_preds)\n",
    "val_accuracy = accuracy_score(y_test, val_preds)\n",
    "\n",
    "print(\"Training Accuracy: {:.4}%\".format(training_accuracy * 100))\n",
    "print(\"Validation accuracy: {:.4}%\".format(val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a big improvement! We've increased our validation accuracy by around 10%, and we've also stopped the model from overfitting.  \n",
    "\n",
    "(**The above comment was supplied by the lab author.  Actually, we only improved validation accuracy by 4.5%... which is still pretty good.  I suspect I could get a better result with more values in the GridSearch.  But the point of the lab is to show improvement, which I have done.**)\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Great! We've now successfully made use of one of the most powerful Boosting models in data science for modeling.  We've also learned how to tune the model for better performance using the Grid Search methodology we learned previously.  XGBoost is a powerful modeling tool to have in your arsenal.  Don't be afraid to experiment with it when modeling."
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('learn-env': conda)",
   "language": "python",
   "name": "python37664bitlearnenvconda70f0b4d48d474c1cba77a4611cdb5f3c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}